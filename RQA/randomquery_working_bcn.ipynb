{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "# # Set the seed for reproducibility\n",
        "# np.random.seed(42)\n",
        "\n",
        "# # Generate 1000 rows of random data for 30 columns\n",
        "# random_data = np.random.rand(100000, 30)\n",
        "\n",
        "# # Convert to a DataFrame for better visualization\n",
        "# random_df = pd.DataFrame(random_data, columns=[f'Feature_{i+1}' for i in range(30)])\n",
        "\n",
        "# # Show the first few rows of the generated data\n",
        "\n"
      ],
      "metadata": {
        "id": "h8seJjVok-o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.random.exponential(scale=1, size=(100000, 30))"
      ],
      "metadata": {
        "id": "J7ly6ksVRc36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "np.random.seed(42)\n",
        "#Random generations for brest cancer data\n",
        "# Generate data from a Poisson distribution with lambda=3\n",
        "# random_data = np.random.poisson(lam=3, size=(1000, 30))\n",
        "# random_data = np.random.exponential(scale=1, size=(1000, 30))\n",
        "random_data = np.abs(np.random.beta(a=2, b=5, size=(50000, 30)))\n",
        "# random_data = np.random.gamma(shape=2, scale=1, size=(1000, 30))\n",
        "# random_data = np.random.lognormal(mean=0, sigma=1, size=(1000, 30))\n",
        "\n",
        "\n",
        "\n",
        "#Random generations for Adult\n",
        "# Generate data from a Poisson distribution with lambda=3\n",
        "# random_data = np.random.poisson(lam=3, size=(1000, 30))\n",
        "# random_data = np.random.exponential(scale=1, size=(1000, 30))\n",
        "random_data = np.abs(np.random.beta(a=2, b=5, size=(50000, 30)))\n",
        "# random_data = np.random.gamma(shape=2, scale=1, size=(1000, 30))\n",
        "# random_data = np.random.lognormal(mean=0, sigma=1, size=(1000, 30))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "random_data = scaler.fit_transform(random_data)\n",
        "\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "random_df = pd.DataFrame(random_data, columns=[f'Feature_{i+1}' for i in range(30)])\n",
        "\n",
        "# Show the first few rows of the generated data\n",
        "print(random_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QBYx8RE5EZq",
        "outputId": "eb2335e3-d141-4e6f-925a-4e4a8dd5e189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
            "0   0.422653  -0.234206   0.816211  -0.782130   1.652912  -1.089854   \n",
            "1  -1.157738  -0.489794   0.148814  -1.503567  -1.022466  -0.462594   \n",
            "2  -0.956037   0.717847   0.230155   0.044246  -1.091610   0.355854   \n",
            "3  -0.321146  -0.694249  -0.116749  -0.290701   0.242099   0.994843   \n",
            "4   0.373695   1.581784   0.127060  -0.871884   1.034524   0.471068   \n",
            "\n",
            "   Feature_7  Feature_8  Feature_9  Feature_10  ...  Feature_21  Feature_22  \\\n",
            "0   1.388726  -0.680768  -0.546147    0.572486  ...   -0.137895    0.651692   \n",
            "1   0.443728   2.291035  -0.771392   -1.350175  ...    0.506479    0.078924   \n",
            "2  -0.728537   3.224743  -0.962065   -0.998775  ...   -1.495637   -0.991350   \n",
            "3   0.614270  -1.188306  -0.479281   -0.743747  ...   -1.341245    0.514400   \n",
            "4  -1.401832  -0.332989  -0.584963    1.472820  ...    0.238462   -0.652524   \n",
            "\n",
            "   Feature_23  Feature_24  Feature_25  Feature_26  Feature_27  Feature_28  \\\n",
            "0   -0.828543    0.123762   -0.514409   -0.645921    0.515493   -0.519703   \n",
            "1   -0.390577   -0.764046    1.102322   -1.183451   -0.663276    0.364836   \n",
            "2    0.251701   -1.048015   -1.112977    0.238224   -0.412572    0.124896   \n",
            "3   -0.757609    2.266880    1.363791    0.766269    0.582051    1.229769   \n",
            "4   -1.098996   -1.403322    0.651192   -0.413864   -0.717471   -1.363686   \n",
            "\n",
            "   Feature_29  Feature_30  \n",
            "0   -0.616304   -0.010911  \n",
            "1   -0.061943    0.993240  \n",
            "2    0.748039   -0.635566  \n",
            "3    1.039388    0.186020  \n",
            "4   -0.490597    0.462146  \n",
            "\n",
            "[5 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x78gd5QQ0aOg",
        "outputId": "81bb566f-b705-42e7-dfa6-56d666fbf6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4', 'Feature_5',\n",
              "       'Feature_6', 'Feature_7', 'Feature_8', 'Feature_9', 'Feature_10',\n",
              "       'Feature_11', 'Feature_12', 'Feature_13', 'Feature_14', 'Feature_15',\n",
              "       'Feature_16', 'Feature_17', 'Feature_18', 'Feature_19', 'Feature_20',\n",
              "       'Feature_21', 'Feature_22', 'Feature_23', 'Feature_24', 'Feature_25',\n",
              "       'Feature_26', 'Feature_27', 'Feature_28', 'Feature_29', 'Feature_30'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming random_df has an 'output' column with binary or categorical values\n",
        "# # First, check the distribution of the output column\n",
        "# print(random_df['output'].value_counts())\n",
        "\n",
        "# # Determine the number of classes in the output column\n",
        "# num_classes = random_df['output'].nunique()\n",
        "\n",
        "# # Sample rows equally from each class\n",
        "# sample_size = 1000 // num_classes  # Calculate how many rows to sample per class\n",
        "\n",
        "# # Perform stratified sampling from each class to get an equal distribution\n",
        "# sampled_df = random_df.groupby('output').sample(n=sample_size, random_state=42)\n",
        "\n",
        "# # Check the sampled distribution\n",
        "# print(sampled_df['output'].value_counts())\n",
        "\n",
        "# # If the desired sample size is not exactly divisible, you can use this to randomly sample:\n",
        "# if len(sampled_df) > 1000:\n",
        "#     sampled_df = sampled_df.sample(n=1000, random_state=42)\n",
        "\n",
        "# # The final sampled_df contains 1000 rows with equal distribution of the 'output' column\n",
        "# print(sampled_df)\n"
      ],
      "metadata": {
        "id": "bXiLgECBvvu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVNyVJwCkv01"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler, MaxAbsScaler, QuantileTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "SYNTHETIC_SAMPLES = 1000000  # Total synthetic samples\n",
        "CHUNK_SIZE = 100000\n",
        "CLASS_PROPORTIONS = {0: 0.5, 1: 0.5}  # Equal distribution\n",
        "\n",
        "class VictimModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(VictimModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "    data = load_breast_cancer()\n",
        "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "    y = pd.Series(data.target)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "\n",
        "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test, scaler\n",
        "\n",
        "def train_model(model, criterion, optimizer, X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            batch_X = X_train[i:i+batch_size]\n",
        "            batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = torch.sigmoid(model(X_test)).numpy().flatten()\n",
        "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    f1 = f1_score(y_test, y_pred_binary)\n",
        "\n",
        "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    # print(f\"Precision: {precision:.4f}\")\n",
        "    # print(f\"Recall: {recall:.4f}\")\n",
        "    # print(f\"F1 Score: {f1:.4f}\")\n",
        "    return y_pred_binary\n",
        "\n",
        "def query_teacher_model(model, data):\n",
        "    # st_scaler = StandardScaler()\n",
        "    # data_scaled = st_scaler.fit_transform(data)\n",
        "    data_scaled = data\n",
        "\n",
        "    data_tensor = torch.tensor(data_scaled, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        outputs = torch.sigmoid(model(data_tensor)).numpy().flatten()\n",
        "        outputs = (outputs >= 0.97).astype(int)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VictimModel(30, 1)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "5FpAYLHIm8yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_tensor, y_train_tensor, X_test_tensor, y_test, scaler=load_and_preprocess_data()\n",
        "# train_model(VictimModel, criterion, optimizer, X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "wRuq5Hx5lyRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "X_train_tensor, y_train_tensor, X_test_tensor, y_test, scaler = load_and_preprocess_data()\n",
        "\n",
        "# Pass the correct tensor to the train_model function (X_train_tensor)\n",
        "train_model(model, criterion, optimizer, X_train_tensor, y_train_tensor, epochs=EPOCHS, batch_size=BATCH_SIZE)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj4dwnLJpJhO",
        "outputId": "fe674a96-dd6a-4944-8516-f4a2cb363cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.0027\n",
            "Epoch [20/100], Loss: 0.0002\n",
            "Epoch [30/100], Loss: 0.0000\n",
            "Epoch [40/100], Loss: 0.0000\n",
            "Epoch [50/100], Loss: 0.0000\n",
            "Epoch [60/100], Loss: 0.0000\n",
            "Epoch [70/100], Loss: 0.0000\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [90/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random_df['output']"
      ],
      "metadata": {
        "id": "Fn9hD5vDPv3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# random_df['output']=list(query_teacher_model(model, random_df.iloc[:, :-1])) # Select all columns except the last one\n",
        "# Ensure you're passing all 30 columns of random_df to the model\n",
        "random_df['output'] = list(query_teacher_model(model, random_df.values))  # Use all columns\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VLd_ItOXsniL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random_df.to_csv('/content/drive/MyDrive/Colab Notebooks/my_dataframe.csv', index=False)"
      ],
      "metadata": {
        "id": "-oj7d9eSrtm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bcn = load_breast_cancer()\n",
        "features = list(bcn.feature_names)\n",
        "\n",
        "features.append('output')\n",
        "\n",
        "\n",
        "print (len(features))\n",
        "\n",
        "\n",
        "random_df.columns = features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQovBodp5Jr_",
        "outputId": "3da642cd-f73a-41db-eace-b660b20c502c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target variable (y)\n",
        "y = random_df['output']\n",
        "X = random_df.drop('output', axis=1)\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets (e.g., 80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n"
      ],
      "metadata": {
        "id": "AHWg1dtmtVDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "MVqPKq-Xte_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfr = RandomForestClassifier()\n",
        "rfr.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "TkZwqNJKtlW7",
        "outputId": "0085e72d-604b-49c9-8902-3cb66efb34ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "IDld8ZCB4WN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred =rfr.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLxvZwZ7uFLx",
        "outputId": "16221264-c6b7-475e-b8a4-da4cc600cc43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.37719298245614036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYarZWmg3LDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate fidelity\n",
        "fidelity_outputs = evaluate_model(model, X_test_tensor, y_test)\n",
        "print(\"teacher Accuracy:\", accuracy_score(fidelity_outputs, y_test))\n",
        "print(\" surrogate Accuracy:\",accuracy_score(y_pred, y_test))\n",
        "print(\"Fidelity Accuracy:\", accuracy_score(fidelity_outputs, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEeBmMNzyPiY",
        "outputId": "1cd94cf7-b8dd-4409-f327-5a10b3ce1c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "teacher Accuracy: 0.9824561403508771\n",
            " surrogate Accuracy: 0.37719298245614036\n",
            "Fidelity Accuracy: 0.37719298245614036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, X_test_tensor, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KriEVI6N1gLc",
        "outputId": "401cb804-0e97-4787-df3e-f78c148a2286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
              "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
              "       0, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPvHlNpY1jKT",
        "outputId": "ab17b129-6488-4623-eb4d-d5911ae9ea11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    }
  ]
}