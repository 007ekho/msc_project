{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "# # Set the seed for reproducibility\n",
        "# np.random.seed(42)\n",
        "\n",
        "# # Generate 1000 rows of random data for 30 columns\n",
        "# random_data = np.random.rand(100000, 30)\n",
        "\n",
        "# # Convert to a DataFrame for better visualization\n",
        "# random_df = pd.DataFrame(random_data, columns=[f'Feature_{i+1}' for i in range(30)])\n",
        "\n",
        "# # Show the first few rows of the generated data\n",
        "\n"
      ],
      "metadata": {
        "id": "h8seJjVok-o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.random.exponential(scale=1, size=(100000, 30))"
      ],
      "metadata": {
        "id": "J7ly6ksVRc36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fetch_openml"
      ],
      "metadata": {
        "id": "E5m9BfKxU67S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.datasets import load_breast_cancer\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# #Random generations for Adult\n",
        "# # Generate data from a Poisson distribution with lambda=3\n",
        "# # random_data = np.random.poisson(lam=3, size=(1000, 30))\n",
        "# # random_data = np.random.exponential(scale=1, size=(1000, 30))\n",
        "# random_data = np.abs(np.random.beta(a=2, b=5, size=(50000, 30)))\n",
        "# # random_data = np.random.gamma(shape=2, scale=1, size=(1000, 30))\n",
        "# # random_data = np.random.lognormal(mean=0, sigma=1, size=(1000, 30))\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# random_data = scaler.fit_transform(random_data)\n",
        "\n",
        "\n",
        "# # Convert to DataFrame for better visualization\n",
        "# random_df = pd.DataFrame(random_data, columns=[f'Feature_{i+1}' for i in range(30)])\n",
        "\n",
        "# # Show the first few rows of the generated data\n",
        "# print(random_df.head())"
      ],
      "metadata": {
        "id": "-QBYx8RE5EZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random_df.columns"
      ],
      "metadata": {
        "id": "x78gd5QQ0aOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVNyVJwCkv01"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler, MaxAbsScaler, QuantileTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "SYNTHETIC_SAMPLES = 1000000  # Total synthetic samples\n",
        "CHUNK_SIZE = 100000\n",
        "CLASS_PROPORTIONS = {0: 0.5, 1: 0.5}  # Equal distribution\n",
        "\n",
        "class VictimModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(VictimModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def load_and_preprocess_data():\n",
        "\n",
        "\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(adult_X_encoded, adult_y_encoded, test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "\n",
        "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test, scaler\n",
        "\n",
        "def train_model(model, criterion, optimizer, X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            batch_X = X_train[i:i+batch_size]\n",
        "            batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = torch.sigmoid(model(X_test)).numpy().flatten()\n",
        "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    f1 = f1_score(y_test, y_pred_binary)\n",
        "\n",
        "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    # print(f\"Precision: {precision:.4f}\")\n",
        "    # print(f\"Recall: {recall:.4f}\")\n",
        "    # print(f\"F1 Score: {f1:.4f}\")\n",
        "    return y_pred_binary\n",
        "\n",
        "def query_teacher_model(model, data):\n",
        "    # st_scaler = StandardScaler()\n",
        "    # data_scaled = st_scaler.fit_transform(data)\n",
        "    data_scaled = data\n",
        "\n",
        "    data_tensor = torch.tensor(data_scaled, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        outputs = torch.sigmoid(model(data_tensor)).numpy().flatten()\n",
        "        outputs = (outputs >= 0.97).astype(int)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VictimModel(14, 1)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "5FpAYLHIm8yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Load the Adult dataset using fetch_openml\n",
        "adult_data = fetch_openml(name='adult', version=2, as_frame=True)\n",
        "\n",
        "# Extract features and target\n",
        "# X = adult_data.data\n",
        "# y = adult_data.target\n",
        "\n",
        "def encode_categorical_data(X, y=None):\n",
        "    # Create a copy of the dataframe to avoid modifying the original data\n",
        "    X_encoded = X.copy()\n",
        "\n",
        "    # Identify categorical columns\n",
        "    categorical_cols = X_encoded.select_dtypes(include=['category', 'object']).columns\n",
        "\n",
        "    # Apply LabelEncoder to each categorical column\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        X_encoded[col] = le.fit_transform(X_encoded[col])\n",
        "\n",
        "    # If target `y` is provided, encode it as well\n",
        "    y_encoded = None\n",
        "    if y is not None:\n",
        "        le_target = LabelEncoder()\n",
        "        y_encoded = le_target.fit_transform(y)\n",
        "\n",
        "    return X_encoded, y_encoded\n",
        "\n",
        "# Example usage:\n",
        "# adult, cancer, diabetes, arrhythmia = load_datasets()\n",
        "\n",
        "# Apply encoding to the features and target:\n",
        "adult_X_encoded, adult_y_encoded = encode_categorical_data(adult_data.data, adult_data.target)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wRuq5Hx5lyRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3238b614-b28c-4ae6-f279-bd083807e1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "source": [
        "X_train_tensor, y_train_tensor, X_test_tensor, y_test, scaler = load_and_preprocess_data()\n",
        "\n",
        "# Pass the correct tensor to the train_model function (X_train_tensor)\n",
        "train_model(model, criterion, optimizer, X_train_tensor, y_train_tensor, epochs=EPOCHS, batch_size=BATCH_SIZE)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj4dwnLJpJhO",
        "outputId": "03d7cc87-4672-4f19-8c12-b9e5a81e092b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.5788\n",
            "Epoch [20/100], Loss: 0.2242\n",
            "Epoch [30/100], Loss: 0.0705\n",
            "Epoch [40/100], Loss: 0.0178\n",
            "Epoch [50/100], Loss: 0.0041\n",
            "Epoch [60/100], Loss: 0.0434\n",
            "Epoch [70/100], Loss: 0.0005\n",
            "Epoch [80/100], Loss: 0.0026\n",
            "Epoch [90/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "#Random generations for Adult\n",
        "# Generate data from a Poisson distribution with lambda=3\n",
        "# random_data = np.random.poisson(lam=3, size=(50000, 14))\n",
        "# random_data = np.random.exponential(scale=1, size=(50000, 14))\n",
        "random_data = np.abs(np.random.beta(a=2, b=5, size=(100000, 14)))\n",
        "# random_data = np.random.gamma(shape=2, scale=1, size=(50000, 14))\n",
        "# random_data = np.random.lognormal(mean=0, sigma=1, size=(50000, 14) )\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# random_data = scaler.fit_transform(random_data)\n",
        "\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "random_df = pd.DataFrame(random_data, columns=[f'Feature_{i+1}' for i in range(14)])\n",
        "\n",
        "# Show the first few rows of the generated data\n",
        "print(random_df.head())"
      ],
      "metadata": {
        "id": "Fn9hD5vDPv3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4904b5d-763c-4163-d87e-ab259b060db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
            "0   0.618728   0.449269   0.241348   0.487066   0.539480   0.248739   \n",
            "1   0.194043   0.212988   0.053584   0.161021   0.130459   0.179439   \n",
            "2   0.104466   0.034650   0.366371   0.155250   0.246090   0.094495   \n",
            "3   0.449679   0.323124   0.210261   0.301146   0.146238   0.346824   \n",
            "4   0.274834   0.068792   0.675472   0.301102   0.080628   0.325356   \n",
            "\n",
            "   Feature_7  Feature_8  Feature_9  Feature_10  Feature_11  Feature_12  \\\n",
            "0   0.410847   0.170424   0.232191    0.425730    0.506732    0.377936   \n",
            "1   0.055322   0.164509   0.369215    0.245466    0.013464    0.320175   \n",
            "2   0.272319   0.073757   0.106786    0.330492    0.423615    0.192832   \n",
            "3   0.127214   0.439963   0.105383    0.078074    0.246728    0.162399   \n",
            "4   0.168723   0.507981   0.381813    0.626030    0.350164    0.269793   \n",
            "\n",
            "   Feature_13  Feature_14  \n",
            "0    0.335028    0.367253  \n",
            "1    0.295385    0.335329  \n",
            "2    0.147905    0.151339  \n",
            "3    0.300127    0.051700  \n",
            "4    0.258697    0.334730  \n"
          ]
        }
      ]
    },
    {
      "source": [
        "# random_df['output'] = list(query_teacher_model(model, random_df.values))  # Use all columns\n",
        "random_df['output'] = list(query_teacher_model(model, random_df.values))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VLd_ItOXsniL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # random_df.to_csv('/content/drive/MyDrive/Colab Notebooks/my_dataframe.csv', index=False)"
      ],
      "metadata": {
        "id": "-oj7d9eSrtm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "features = list(adult_X_encoded.columns)\n",
        "\n",
        "features.append('output')\n",
        "\n",
        "\n",
        "print (len(features))\n",
        "\n",
        "\n",
        "random_df.columns = features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQovBodp5Jr_",
        "outputId": "ee82e612-7db0-4696-ff84-39400c5e98dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target variable (y)\n",
        "y = random_df['output']\n",
        "X = random_df.drop('output', axis=1)\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets (e.g., 80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n"
      ],
      "metadata": {
        "id": "AHWg1dtmtVDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "MVqPKq-Xte_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfr = RandomForestClassifier()\n",
        "rfr.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "TkZwqNJKtlW7",
        "outputId": "939f1886-fb6a-4f7d-9c6f-2b2f52c9eaf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_breast_cancer()\n",
        "X = adult_X_encoded\n",
        "y = adult_y_encoded\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "IDld8ZCB4WN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred =rfr.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLxvZwZ7uFLx",
        "outputId": "8beaeb98-8a1b-4c3a-8afd-39b2e1dd9f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7796089671409561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYarZWmg3LDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate fidelity\n",
        "fidelity_outputs = evaluate_model(model, X_test_tensor, y_test)\n",
        "print(\"teacher Accuracy:\", accuracy_score(fidelity_outputs, y_test))\n",
        "print(\" surrogate Accuracy:\",accuracy_score(y_pred, y_test))\n",
        "print(\"Fidelity Accuracy:\", accuracy_score(fidelity_outputs, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEeBmMNzyPiY",
        "outputId": "9f4c5d3c-0c1b-435c-8c7f-b00d90b58256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "teacher Accuracy: 0.8283345275872659\n",
            " surrogate Accuracy: 0.7796089671409561\n",
            "Fidelity Accuracy: 0.7866721261132152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, X_test_tensor, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KriEVI6N1gLc",
        "outputId": "364863ab-4ca8-49f2-dd7a-c886489ab6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPvHlNpY1jKT",
        "outputId": "2b4161a5-721b-4908-a31c-5283c1eae886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    }
  ]
}